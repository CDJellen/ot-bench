{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A guided overview of `otbench`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an overview of the `otbench` package, including the motivation and key design decisions of this project.\n",
    "\n",
    "An interactive example focused on forecasting using the USNA $C_n^2$ small dataset is available [here](/notebooks/forecasting/usna_cn2_sm.ipynb).\n",
    "A similar interactive example focued on regression using the MLO $C_n^2$ dataset is provided [here](/notebooks/regression/mlo_cn2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from otbench.tasks import TaskApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprinter = pprint.PrettyPrinter(indent=4, width=120, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optical turbulence benchmark package provides a set of tasks to be used for the evaluation of optical turbulence prediction methods.\n",
    "\n",
    "The package includes both regression tasks, and forecasting tasks. Each regression task assesses a given model's ability to predict the optical turbulence strength (as measured by $C_n^2$) at a given location, given a set of meterological parameters. Each forecasting task assesses a given model's ability to predict the optical turbulence strength some number of time steps in the future given a set of prior meterological parameters and previous measurements of the optical turbulence strength at that location.\n",
    "\n",
    "Each task contains a dataset and a set of metrics.\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "Datasets contain some number of timestamped observations of the optical turbulence strength alongside a set of potentially relevant meterological and oceanographic parameters. Datasets may or may not contain missing measurements, and strive to conform to the [NetCDF](https://docs.unidata.ucar.edu/netcdf-c/current/index.html) Climate and Forecast (CF) [Metadata Conventions](http://cfconventions.org/).\n",
    "\n",
    "Under the hood, datasets are stored as [xarray](http://xarray.pydata.org/en/stable/) `Dataset` objects. They are serialized to disk as [NetCDF](https://docs.unidata.ucar.edu/netcdf-c/current/index.html) files. Datasets are shared between one or more tasks. For example, the `mlo_cn2` dataset is used by both the `mlo_cn2` regression task and the `mlo_cn2_forecast` forecasting task.\n",
    "\n",
    "The train, test, and validation splits are defined by the task using fixed indices. Tasks also define data processing pipelines that are applied to the data before it is used for training, testing, or validation. This can include common techniques such as removing rows with missing measurements, or taking the log of the optical turbulence strength. Finally, tasks define the set of features which are unavailable for training. Again taking the `mlo_cn2` tasks as an example, the target feature is the optical turbulence strength at a height of 15 \\[m\\]; the unavailable features are the optical turbulence strength at other heights are assumed as unavailable for training or inference in the regression task.\n",
    "\n",
    "Tasks evaluate the performance of a model on the test and validation splits using the metrics defined by the task.\n",
    "\n",
    "#### Metrics\n",
    "\n",
    "The metrics are used to evaluate the performance of a model or prediction method on the data. Metrics, in the context of `otb` tasks, allow for rigorous comparison of different models and prediction methods. Metrics are defined by the task, and are evaluated on the test and validation splits of the dataset.\n",
    "\n",
    "Many tasks use standard error metrics including:\n",
    "* mean absolute error (MAE)\n",
    "* explained variance score (EVS or $R^2$)\n",
    "* root mean squared error (RMSE)\n",
    "* mean absolute percentage error (MAPE)\n",
    "\n",
    "All regression and forecasting tasks include some baseline models which can be applied to the prediction problem. Each task's metrics are evaluated on the baseline models and the results are stored in a shared `experiments.json` file. This allows for easy comparison of different models and prediction methods. When developing a new model or prediction method, it is recommended to compare the performance of the new method to the baseline models. After the new method has been evaluated, it can be programmatically added to the `experiments.json` file for future comparison using the task's interface.\n",
    "\n",
    "#### Example: (`mlo_cn2`) regression task, without missing values\n",
    "\n",
    "```\n",
    "{\n",
    "    'description': 'Regression task for MLO Cn2 data, ...',\n",
    "    'description_long': 'This dataset evaluates ...',\n",
    "    'dropna': True,\n",
    "    'ds_name': 'mlo_cn2',\n",
    "    'eval_metrics': ['root_mean_square_error', 'coefficient_of_determination', 'mean_absolute_error', 'mean_absolute_percentage_error'],\n",
    "    'log_transform': True,\n",
    "    'obs_lat': 19.53,\n",
    "    'obs_lon': -155.57,\n",
    "    'obs_tz': 'US/Hawaii',\n",
    "    'remove': ['base_time', 'Cn2_6m', 'Cn2_15m', 'Cn2_25m'],\n",
    "    'target': 'Cn2_15m',\n",
    "    'val_idx': ['8367:10367'],\n",
    "    'train_idx': ['0:8367'],\n",
    "    'test_idx': ['10367:13943']\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TaskApi` is the main entry point for the `otb` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_api = TaskApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TaskApi` provides access to the tasks, which in turn enable access to training, test, and validation data, benchmarking metrics, and evaluation of new prediction models or methods.\n",
    "\n",
    "The tasks which are currently supported by the `otb` package are accessible via the `TaskApi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forecasting.usna_cn2_sm.full.Cn2_3m',\n",
       " 'regression.usna_cn2_lg.full.Cn2_3m',\n",
       " 'forecasting.mlo_cn2.dropna.Cn2_15m',\n",
       " 'regression.usna_cn2_sm.full.Cn2_3m',\n",
       " 'regression.mlo_cn2.dropna.Cn2_15m',\n",
       " 'regression.mlo_cn2.full.Cn2_15m']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_api.list_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustrative example, we can load the `mlo_cn2` regression task with missing values removed and develop a new model for predicting optical turbulence strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = task_api.get_task(\"regression.mlo_cn2.dropna.Cn2_15m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `task` object gives access to the description and associated metadata surrounding the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'description': 'Regression task for MLO Cn2 data, where the last 12 days are set aside for evaluation',\n",
      "    'description_long': 'This dataset evaluates regression approaches for predicting the extent of optical turbulence, '\n",
      "                        'as measured by Cn2 at an elevation of 15m. Optical turbulence on data collected at the Mauna '\n",
      "                        'Loa Solar Observatory between 27 July 2006 and 8 August 2006, inclusive, are used to evaluate '\n",
      "                        'prediction accuracy under the root-mean square error metric.',\n",
      "    'dropna': True,\n",
      "    'ds_name': 'mlo_cn2',\n",
      "    'eval_metrics': ['root_mean_square_error', 'coefficient_of_determination', 'mean_absolute_error', 'mean_absolute_percentage_error'],\n",
      "    'log_transform': True,\n",
      "    'obs_lat': 19.53,\n",
      "    'obs_lon': -155.57,\n",
      "    'obs_tz': 'US/Hawaii',\n",
      "    'remove': ['base_time', 'Cn2_6m', 'Cn2_15m', 'Cn2_25m'],\n",
      "    'target': 'Cn2_15m',\n",
      "    'test_idx': ['10367:13943'],\n",
      "    'train_idx': ['0:8367'],\n",
      "    'val_idx': ['8367:10367']}\n"
     ]
    }
   ],
   "source": [
    "task_info = task.get_info()\n",
    "pprinter.pprint(task_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the `regression.mlo_cn2.dropna.Cn2_15m` task is focused on predicting the optical turbulence strength at a height of 15 \\[m\\] at the Mauna Loa Observatory (MLO) in Hawaii. The task uses the `mlo_cn2` dataset, which is a dataset of optical turbulence strength measurements at the MLO. The `task` contains an `obs_tz` attribute which specifies the timezone of the observatory. The latitude and longitude of the observatory are also provided as `obs_lat` and `obs_lon` attributes.\n",
    "\n",
    "The `task` also contains a `target` attribute which specifies the target feature for the task. The task is focused on predicting the optical turbulence strength at a height of 15 \\[m\\], and the optical turbulence strength measurements at heights of 6 and 25 \\[m\\] are assumed to be unavailable for training or inference.\n",
    "\n",
    "To ensure consistency and robust comparison between modeling approaches, the `train_idx`, `test_idx`, and `val_idx` are fixed for the given task. The `train_idx` and `val_idx` attributes specify the indices of the dataset which are available for model development. The `test_idx` attribute specifies the indices of the dataset which are used to evaluate the model during and compare against existing benchmarks for the task.\n",
    "\n",
    "The task is evaluated using the root mean squared error (RMSE), explained variance score (EVS), mean absolute error (MAE), and mean absolute percentage error (MAPE) metrics. The task is evaluated on the test and validation splits of the dataset, and the training split is used for training new models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = task.get_train_data(data_type=\"pd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `otb` package attempts to make as few assumptions about the model or prediction method's API surface as possible. A major constraint is the assumption that each model is called during evaluation against the validation set in the same form as is returned by the `get_training_data` method with the `data_type` argument set to `pd`.\n",
    "\n",
    "Models can take many forms, from simple statistical models such as predicting the mean value seen during training, to complex deep learning models. The `otb` package does not attempt to provide a unified API for developing all models, but instead provides a set of tools for evaluating models against the tasks.\n",
    "\n",
    "Existing statistical and parametric techniques are included under the `otb.benchmark.models` module. These models provide samples of best practices for developing new models for the tasks. An example statistical method which predicts the mean value seen during training is included below.\n",
    "\n",
    "```python\n",
    "class PersistanceRegressionModel:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.mean = np.nan\n",
    "    \n",
    "    def train(self, X: 'pd.DataFrame', y: Union['pd.DataFrame', 'pd.Series', np.ndarray]):\n",
    "        # maintain the same interface as the other models\n",
    "        self.mean = np.mean(y)\n",
    "\n",
    "    def predict(self, X: 'pd.DataFrame'):\n",
    "        # predict the mean for each entry in X\n",
    "        return np.full(len(X), self.mean)\n",
    "```\n",
    "\n",
    "When evaluated, the `PersistanceRegressionModel`s performance is measured by calling the `predict` method on the validation data and comparing the results to the ground truth values. The `PersistanceRegressionModel` has already been evaluated against the metrics defined by the task, and the results are stored in the `experiments.json` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on using the `otbench` package to evaluate new models can be found in the [regression overview](regression/modeling.ipynb) notebook and the [forecasting overview](forecasting/usna_cn2_sm.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
